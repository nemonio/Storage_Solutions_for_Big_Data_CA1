{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4bce9b",
   "metadata": {},
   "source": [
    "## Ensure Spark is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55874526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.4\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fc838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "# Check the master URL of the SparkContext\n",
    "print(sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2fe43",
   "metadata": {},
   "source": [
    "# Previous steps (Terminal commands)\n",
    "\n",
    "## A. Run Hadoop using an extended hdfs script created by me using Muhammed's script as a base\n",
    "\n",
    "./hdfs_extended\n",
    "\n",
    "\n",
    "## B. Create a new folder in Hadoop File Sytem called \"CA1\"\n",
    "\n",
    "hadoop fs -mkdir /CA1\n",
    "\n",
    "\n",
    "## C. Copy my dataset with TAB separators instead of commas to the folder I've just created\n",
    "\n",
    "hadoop fs -put /media/sf_CA1/lyrics-dataTAB.csv /CA1\n",
    "\n",
    "\n",
    "## D. Ensure it is there\n",
    "\n",
    "hadoop fs -ls /CA1\n",
    "\n",
    "\n",
    "## E. Run a script I created to replace all double quote characters to single quote characters and create a new file called \"lyrics-dataTAB_fixed.csv\"\n",
    "\n",
    "Code for the script:\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "#Define your HDFS file paths\n",
    "input_path=\"/CA1/lyrics-dataTAB.csv\"\n",
    "output_path=\"/CA1/lyrics-dataTAB_fixed.csv\"\n",
    "\n",
    "#Check if the output file already exists and remove it to avoid errors\n",
    "\n",
    "hdfs dfs -test -e $output_path && hdfs dfs -rm $output_path\n",
    "\n",
    "#Use hdfs dfs -cat to output the file content, then sed to replace \" with ' and then use hdfs dfs -put to store the result back into HDFS\n",
    "\n",
    "hdfs dfs -cat $input_path | sed \"s/\\\"/'/g\" | hdfs dfs -put - $output_path\n",
    "\n",
    "\n",
    "Terminal code to tun it:\n",
    "\n",
    "./scriptToReplaceDoubleQuotesWithSingleQuotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc60a34",
   "metadata": {},
   "source": [
    "# DATA-PREPROCESSING with Spark over file in Hadoop File System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913805f",
   "metadata": {},
   "source": [
    "# 01. Load csv with tab separator from hadoop filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa6c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+--------+\n",
      "|          ALink|               SName|               SLink|               Lyric|language|\n",
      "+---------------+--------------------+--------------------+--------------------+--------+\n",
      "|/ivete-sangalo/|               Arerê|/ivete-sangalo/ar...|'Tudo o que eu qu...|      pt|\n",
      "|/ivete-sangalo/|Se Eu Não Te Amas...|/ivete-sangalo/se...|'Meu coração\\nSem...|      pt|\n",
      "|/ivete-sangalo/|         Céu da Boca|/ivete-sangalo/ch...|'É de babaixá!\\nÉ...|      pt|\n",
      "|/ivete-sangalo/|Quando A Chuva Pa...|/ivete-sangalo/qu...|'Quando a chuva p...|      pt|\n",
      "|/ivete-sangalo/|        Sorte Grande|/ivete-sangalo/so...|'A minha sorte gr...|      pt|\n",
      "|/ivete-sangalo/|    A Lua Q Eu T Dei|/ivete-sangalo/a-...|'Posso te falar d...|      pt|\n",
      "|/ivete-sangalo/|Mulheres Não Têm ...|/ivete-sangalo/mu...|'Hey, girl\\nLevan...|      pt|\n",
      "|/ivete-sangalo/|Eva / Alô Paixão ...|/ivete-sangalo/ev...|'''EVA''\\n(Gianca...|      pt|\n",
      "|/ivete-sangalo/|      Flor do Reggae|/ivete-sangalo/fl...|'Um brilho de amo...|      pt|\n",
      "|/ivete-sangalo/|         Carro Velho|/ivete-sangalo/ca...|'Cheiro de pneu q...|      pt|\n",
      "|/ivete-sangalo/|  Muito Obrigado Axé|/ivete-sangalo/mu...|'Odô, axé odô, ax...|      pt|\n",
      "|/ivete-sangalo/|   Não Precisa Mudar|/ivete-sangalo/na...|'Não precisa muda...|      pt|\n",
      "|/ivete-sangalo/|Nada Vai Nos Separar|/ivete-sangalo/na...|'Toda vez que eu ...|      pt|\n",
      "|/ivete-sangalo/|    Tempo de Alegria|/ivete-sangalo/te...|'É amor\\nÉ tanto ...|      pt|\n",
      "|/ivete-sangalo/|        Agora Já Sei|/ivete-sangalo/ag...|'Duvidava, não en...|      pt|\n",
      "|/ivete-sangalo/|               Deixo|/ivete-sangalo/de...|'Eu me lembro sem...|      pt|\n",
      "|/ivete-sangalo/|Não Me Conte Seus...|/ivete-sangalo/na...|'Ivete Sangaloooo...|      pt|\n",
      "|/ivete-sangalo/|País Tropical / A...|/ivete-sangalo/pa...|'Moro...\\nNum paí...|      pt|\n",
      "|/ivete-sangalo/|            Na Bahia|/ivete-sangalo/na...|'Na Bahia ia ia i...|      pt|\n",
      "|/ivete-sangalo/|            Completo|/ivete-sangalo/co...|'É tão bom ter al...|      pt|\n",
      "+---------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows is: 389622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"hdfs:///CA1/lyrics-dataTAB_fixed.csv\", sep='\\t', header=True, multiLine = True, escape = \"\\n\")\n",
    "\n",
    "df.show(20)\n",
    "\n",
    "# Count the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Show the row count\n",
    "print(f\"The total number of rows is: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e378f",
   "metadata": {},
   "source": [
    "# 02. Drop rows that contain some null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e286722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows is: 364924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop rows that have any null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Count the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Show the row count\n",
    "print(f\"The total number of rows is: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd382ee",
   "metadata": {},
   "source": [
    "# 03. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e646db6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows is: 364924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# To drop identical rows across all columns\n",
    "df_no_identical = df.distinct()\n",
    "\n",
    "# Count the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Show the row count\n",
    "print(f\"The total number of rows is: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b950b2",
   "metadata": {},
   "source": [
    "# 04. Replace newline codes (\\n) with spaces in Lyric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c67e777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows is: 364924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Replace newline characters with a space in the \"lyric\" column\n",
    "df = df.withColumn(\"lyric\", regexp_replace(\"lyric\", \"\\n\", \" \"))\n",
    "\n",
    "# Count the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Show the row count\n",
    "print(f\"The total number of rows is: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16e31b",
   "metadata": {},
   "source": [
    "# 05. Drop rows with language codes with more than 2 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c31e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+--------+\n",
      "|          ALink|               SName|               SLink|               lyric|language|\n",
      "+---------------+--------------------+--------------------+--------------------+--------+\n",
      "|/ivete-sangalo/|               Arerê|/ivete-sangalo/ar...|'Tudo o que eu qu...|      pt|\n",
      "|/ivete-sangalo/|Se Eu Não Te Amas...|/ivete-sangalo/se...|'Meu coração Sem ...|      pt|\n",
      "|/ivete-sangalo/|         Céu da Boca|/ivete-sangalo/ch...|'É de babaixá! É ...|      pt|\n",
      "|/ivete-sangalo/|Quando A Chuva Pa...|/ivete-sangalo/qu...|'Quando a chuva p...|      pt|\n",
      "|/ivete-sangalo/|        Sorte Grande|/ivete-sangalo/so...|'A minha sorte gr...|      pt|\n",
      "|/ivete-sangalo/|    A Lua Q Eu T Dei|/ivete-sangalo/a-...|'Posso te falar d...|      pt|\n",
      "|/ivete-sangalo/|Mulheres Não Têm ...|/ivete-sangalo/mu...|'Hey, girl Levant...|      pt|\n",
      "|/ivete-sangalo/|Eva / Alô Paixão ...|/ivete-sangalo/ev...|'''EVA'' (Giancar...|      pt|\n",
      "|/ivete-sangalo/|      Flor do Reggae|/ivete-sangalo/fl...|'Um brilho de amo...|      pt|\n",
      "|/ivete-sangalo/|         Carro Velho|/ivete-sangalo/ca...|'Cheiro de pneu q...|      pt|\n",
      "|/ivete-sangalo/|  Muito Obrigado Axé|/ivete-sangalo/mu...|'Odô, axé odô, ax...|      pt|\n",
      "|/ivete-sangalo/|   Não Precisa Mudar|/ivete-sangalo/na...|'Não precisa muda...|      pt|\n",
      "|/ivete-sangalo/|Nada Vai Nos Separar|/ivete-sangalo/na...|'Toda vez que eu ...|      pt|\n",
      "|/ivete-sangalo/|    Tempo de Alegria|/ivete-sangalo/te...|'É amor É tanto a...|      pt|\n",
      "|/ivete-sangalo/|        Agora Já Sei|/ivete-sangalo/ag...|'Duvidava, não en...|      pt|\n",
      "|/ivete-sangalo/|               Deixo|/ivete-sangalo/de...|'Eu me lembro sem...|      pt|\n",
      "|/ivete-sangalo/|Não Me Conte Seus...|/ivete-sangalo/na...|'Ivete Sangaloooo...|      pt|\n",
      "|/ivete-sangalo/|País Tropical / A...|/ivete-sangalo/pa...|'Moro... Num país...|      pt|\n",
      "|/ivete-sangalo/|            Na Bahia|/ivete-sangalo/na...|'Na Bahia ia ia i...|      pt|\n",
      "|/ivete-sangalo/|            Completo|/ivete-sangalo/co...|'É tão bom ter al...|      pt|\n",
      "+---------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is: 364922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "\n",
    "# Filter rows to keep only those where \"language\" column has <= 2 characters\n",
    "df = df.filter(length(df.language) <= 2)\n",
    "\n",
    "# Show the filtered DataFrame (optional)\n",
    "df.show()\n",
    "\n",
    "# Count the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Show the row count\n",
    "print(f\"The number of rows is: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b120c",
   "metadata": {},
   "source": [
    "# 06. Add a new column to the DataFrame with unique row identifiers (IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaa3a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+--------+---+\n",
      "|          ALink|               SName|               SLink|               lyric|language| id|\n",
      "+---------------+--------------------+--------------------+--------------------+--------+---+\n",
      "|/ivete-sangalo/|               Arerê|/ivete-sangalo/ar...|'Tudo o que eu qu...|      pt|  0|\n",
      "|/ivete-sangalo/|Se Eu Não Te Amas...|/ivete-sangalo/se...|'Meu coração Sem ...|      pt|  1|\n",
      "|/ivete-sangalo/|         Céu da Boca|/ivete-sangalo/ch...|'É de babaixá! É ...|      pt|  2|\n",
      "|/ivete-sangalo/|Quando A Chuva Pa...|/ivete-sangalo/qu...|'Quando a chuva p...|      pt|  3|\n",
      "|/ivete-sangalo/|        Sorte Grande|/ivete-sangalo/so...|'A minha sorte gr...|      pt|  4|\n",
      "|/ivete-sangalo/|    A Lua Q Eu T Dei|/ivete-sangalo/a-...|'Posso te falar d...|      pt|  5|\n",
      "|/ivete-sangalo/|Mulheres Não Têm ...|/ivete-sangalo/mu...|'Hey, girl Levant...|      pt|  6|\n",
      "|/ivete-sangalo/|Eva / Alô Paixão ...|/ivete-sangalo/ev...|'''EVA'' (Giancar...|      pt|  7|\n",
      "|/ivete-sangalo/|      Flor do Reggae|/ivete-sangalo/fl...|'Um brilho de amo...|      pt|  8|\n",
      "|/ivete-sangalo/|         Carro Velho|/ivete-sangalo/ca...|'Cheiro de pneu q...|      pt|  9|\n",
      "|/ivete-sangalo/|  Muito Obrigado Axé|/ivete-sangalo/mu...|'Odô, axé odô, ax...|      pt| 10|\n",
      "|/ivete-sangalo/|   Não Precisa Mudar|/ivete-sangalo/na...|'Não precisa muda...|      pt| 11|\n",
      "|/ivete-sangalo/|Nada Vai Nos Separar|/ivete-sangalo/na...|'Toda vez que eu ...|      pt| 12|\n",
      "|/ivete-sangalo/|    Tempo de Alegria|/ivete-sangalo/te...|'É amor É tanto a...|      pt| 13|\n",
      "|/ivete-sangalo/|        Agora Já Sei|/ivete-sangalo/ag...|'Duvidava, não en...|      pt| 14|\n",
      "|/ivete-sangalo/|               Deixo|/ivete-sangalo/de...|'Eu me lembro sem...|      pt| 15|\n",
      "|/ivete-sangalo/|Não Me Conte Seus...|/ivete-sangalo/na...|'Ivete Sangaloooo...|      pt| 16|\n",
      "|/ivete-sangalo/|País Tropical / A...|/ivete-sangalo/pa...|'Moro... Num país...|      pt| 17|\n",
      "|/ivete-sangalo/|            Na Bahia|/ivete-sangalo/na...|'Na Bahia ia ia i...|      pt| 18|\n",
      "|/ivete-sangalo/|            Completo|/ivete-sangalo/co...|'É tão bom ter al...|      pt| 19|\n",
      "+---------------+--------------------+--------------------+--------------------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add a new column 'id' with unique row IDs\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Show the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66915568",
   "metadata": {},
   "source": [
    "# 07. Save \"Lyrics-dataForHbase.csv\" with tab separator to hadoop filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ba6178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the limited DataFrame to a new CSV file\n",
    "df.write.option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(\"hdfs:///CA1/lyrics-dataForHbase.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddd83c",
   "metadata": {},
   "source": [
    "# 08. Import preprocessed file from Hadoop into HBase (Terminal commands)\n",
    "\n",
    "## 8.1 Ensure \"lyrics-dataForHbase.csv\" is in hadoop filesystem\n",
    "\n",
    "hadoop fs -ls /CA1\n",
    "\n",
    "## 8.2 Run hbase shell\n",
    "\n",
    "/usr/local/hbase/bin/hbase shell\n",
    "\n",
    "## 8.3 Create \"Lyrics\" table in hbase shell\n",
    "\n",
    "create 'LyricsTEST', 'cf'\n",
    "\n",
    "## 8.4 Import \"lyrics-dataForHbase.csv\" from hadoop filesystem to HBase\n",
    "\n",
    "bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv \\\n",
    "-Dimporttsv.columns=cf:ALink,cf:SName,cf:SLink,cf:Lyric,cf:language,HBASE_ROW_KEY \\\n",
    "-Dmapreduce.input.fileinputformat.input.charset=UTF-8 \\\n",
    "Lyrics hdfs:///CA1/lyrics-dataForHbase.csv\n",
    "\n",
    "## 8.5 Run Thrift server (needed for Happy Base)\n",
    "\n",
    "/usr/local/hbase/bin/hbase thrift start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f45701",
   "metadata": {},
   "source": [
    "# 09. Access HBase from notebook using HappyBase python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "214d1d02",
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: lyricsTEST\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:989)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:866)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:319)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:157)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:53)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:187)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:266)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:435)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:309)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:594)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:94)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.scannerGetList(ThriftHBaseServiceHandler.java:814)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:63)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:5093)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:5072)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:279)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7996/2955576081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Extract all rows from HBase into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Construct DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7996/2955576081.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Extract all rows from HBase into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Construct DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/happybase/table.py\u001b[0m in \u001b[0;36mscan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0mhow_many\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_returned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 items = self.connection.client.scannerGetList(\n\u001b[0m\u001b[1;32m    415\u001b[0m                     scan_id, how_many)\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/thriftpy2/thrift.py\u001b[0m in \u001b[0;36m_req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# wait result only if non-oneway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oneway\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/thriftpy2/thrift.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"success\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# no throws & not void api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: lyricsTEST\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:989)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:866)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:319)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:157)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:53)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:187)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:266)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:435)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:309)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:594)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:94)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.scannerGetList(ThriftHBaseServiceHandler.java:814)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:63)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:5093)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:5072)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:279)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n')"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to HBase\n",
    "connection = happybase.Connection('localhost')\n",
    "table = connection.table('lyricsTEST')\n",
    "\n",
    "# Extract all rows from HBase into a list\n",
    "rows = [data for _, data in table.scan()]\n",
    "\n",
    "# Construct DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Convert bytes columns to string\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].str.decode('utf-8')\n",
    "\n",
    "df.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c8e6a",
   "metadata": {},
   "source": [
    "## ******************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9247031",
   "metadata": {},
   "source": [
    "## ******************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b24e14",
   "metadata": {},
   "source": [
    "## Appendix 01. Save a reduced dataset to work with before doing the big importing to hbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b7d33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the DataFrame to the first 20 rows\n",
    "df_limit = df.limit(20)\n",
    "\n",
    "# Write the limited DataFrame to a new CSV file\n",
    "#df_limit.write.option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(\"hdfs:///CA1/lyrics-dataQUO20tab.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7920cd",
   "metadata": {},
   "source": [
    "## Appendix 02. Find unique values for column \"Language\" and number of songs for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "604875bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 210:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|result                      |\n",
      "+----------------------------+\n",
      "|English (en) 191576 songs   |\n",
      "|Portuguese (pt) 157276 songs|\n",
      "|Spanish (es) 9905 songs     |\n",
      "|Kinyarwanda (rw) 1678 songs |\n",
      "|Italian (it) 1431 songs     |\n",
      "|French (fr) 1222 songs      |\n",
      "|German (de) 844 songs       |\n",
      "|Finnish (fi) 145 songs      |\n",
      "|Swedish (sv) 112 songs      |\n",
      "|Romanian (ro) 97 songs      |\n",
      "|Norwegian (no) 89 songs     |\n",
      "|Icelandic (is) 86 songs     |\n",
      "|Tagalog (tl) 69 songs       |\n",
      "|Polish (pl) 47 songs        |\n",
      "|Galician (gl) 36 songs      |\n",
      "|Turkish (tr) 32 songs       |\n",
      "|Irish (ga) 32 songs         |\n",
      "|Indonesian (id) 26 songs    |\n",
      "|Welsh (cy) 23 songs         |\n",
      "|Afrikaans (af) 19 songs     |\n",
      "|Swahili (sw) 19 songs       |\n",
      "|Sundanese (su) 19 songs     |\n",
      "|Korean (ko) 17 songs        |\n",
      "|Dutch (nl) 14 songs         |\n",
      "|Estonian (et) 13 songs      |\n",
      "|Danish (da) 13 songs        |\n",
      "|Catalan (ca) 13 songs       |\n",
      "|Malay (ms) 8 songs          |\n",
      "|Japanese (ja) 7 songs       |\n",
      "|Sotho (st) 7 songs          |\n",
      "|Haitian Creole (ht) 5 songs |\n",
      "|Scottish Gaelic (gd) 4 songs|\n",
      "|Russian (ru) 4 songs        |\n",
      "|Basque (eu) 4 songs         |\n",
      "|Arabic (ar) 4 songs         |\n",
      "|Malagasy (mg) 3 songs       |\n",
      "|Chichewa (ny) 3 songs       |\n",
      "|Czech (cs) 3 songs          |\n",
      "|Hungarian (hu) 2 songs      |\n",
      "|Javanese (jw) 2 songs       |\n",
      "|Kurdish (ku) 2 songs        |\n",
      "|Ganda (lg) 2 songs          |\n",
      "|Vietnamese (vi) 1 songs     |\n",
      "|Latvian (lv) 1 songs        |\n",
      "|Persian (fa) 1 songs        |\n",
      "|Serbian (sr) 1 songs        |\n",
      "|Chinese (zh) 1 songs        |\n",
      "|Croatian (hr) 1 songs       |\n",
      "|Albanian (sq) 1 songs       |\n",
      "|Hebrew (iw) 1 songs         |\n",
      "+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, concat_ws\n",
    "\n",
    "# Define the language mapping data\n",
    "language_mapping_data = [\n",
    "    (\"en\", \"English\"),\n",
    "    (\"vi\", \"Vietnamese\"),\n",
    "    (\"ro\", \"Romanian\"),\n",
    "    (\"lv\", \"Latvian\"),\n",
    "    (\"pl\", \"Polish\"),\n",
    "    (\"st\", \"Sotho\"),\n",
    "    (\"pt\", \"Portuguese\"),\n",
    "    (\"gl\", \"Galician\"),\n",
    "    (\"tl\", \"Tagalog\"),\n",
    "    (\"sw\", \"Swahili\"),\n",
    "    (\"ko\", \"Korean\"),\n",
    "    (\"ms\", \"Malay\"),\n",
    "    (\"cs\", \"Czech\"),\n",
    "    (\"mg\", \"Malagasy\"),\n",
    "    (\"sr\", \"Serbian\"),\n",
    "    (\"tr\", \"Turkish\"),\n",
    "    (\"de\", \"German\"),\n",
    "    (\"is\", \"Icelandic\"),\n",
    "    (\"es\", \"Spanish\"),\n",
    "    (\"hr\", \"Croatian\"),\n",
    "    (\"eu\", \"Basque\"),\n",
    "    (\"lg\", \"Ganda\"),\n",
    "    (\"it\", \"Italian\"),\n",
    "    (\"af\", \"Afrikaans\"),\n",
    "    (\"ku\", \"Kurdish\"),\n",
    "    (\"su\", \"Sundanese\"),\n",
    "    (\"ar\", \"Arabic\"),\n",
    "    (\"sv\", \"Swedish\"),\n",
    "    (\"nl\", \"Dutch\"),\n",
    "    (\"rw\", \"Kinyarwanda\"),\n",
    "    (\"hu\", \"Hungarian\"),\n",
    "    (\"ca\", \"Catalan\"),\n",
    "    (\"ru\", \"Russian\"),\n",
    "    (\"iw\", \"Hebrew\"),\n",
    "    (\"ga\", \"Irish\"),\n",
    "    (\"ht\", \"Haitian Creole\"),\n",
    "    (\"no\", \"Norwegian\"),\n",
    "    (\"fa\", \"Persian\"),\n",
    "    (\"cy\", \"Welsh\"),\n",
    "    (\"et\", \"Estonian\"),\n",
    "    (\"zh\", \"Chinese\"),\n",
    "    (\"fr\", \"French\"),\n",
    "    (\"ja\", \"Japanese\"),\n",
    "    (\"gd\", \"Scottish Gaelic\"),\n",
    "    (\"id\", \"Indonesian\"),\n",
    "    (\"ny\", \"Chichewa\"),\n",
    "    (\"da\", \"Danish\"),\n",
    "    (\"sq\", \"Albanian\"),\n",
    "    (\"fi\", \"Finnish\"),\n",
    "    (\"jw\", \"Javanese\"),\n",
    "    # Add more mappings here\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the language mapping data\n",
    "language_mapping_df = spark.createDataFrame(language_mapping_data, [\"code\", \"language_name\"])\n",
    "\n",
    "# Alias the DataFrames to avoid column name ambiguities\n",
    "df_alias = df.alias(\"df\")\n",
    "language_mapping_df_alias = language_mapping_df.alias(\"lm\")\n",
    "\n",
    "# Join the DataFrames on the language code\n",
    "df_with_language_names = df_alias.join(language_mapping_df_alias, col(\"df.language\") == col(\"lm.code\"))\n",
    "\n",
    "# Select the necessary columns using the correct DataFrame alias\n",
    "df_with_language_names = df_with_language_names.select(\n",
    "    col(\"df.ALink\"),\n",
    "    col(\"df.SName\"),\n",
    "    col(\"df.SLink\"),\n",
    "    col(\"df.lyric\"),\n",
    "    col(\"lm.code\").alias(\"language_code\"),\n",
    "    col(\"lm.language_name\")\n",
    ")\n",
    "\n",
    "# Count the number of songs per language, and order by this count\n",
    "language_song_counts = (df_with_language_names\n",
    "                        .groupBy(\"language_code\", \"language_name\")\n",
    "                        .count()\n",
    "                        .orderBy(col(\"count\").desc()))\n",
    "\n",
    "# Show the result, concatenating the strings to match the required format\n",
    "language_song_counts.select(\n",
    "    concat_ws(\" \", \n",
    "              col(\"language_name\"), \n",
    "              concat_ws(\"\", lit(\"(\"), col(\"language_code\"), lit(\")\")), \n",
    "              concat_ws(\"\", col(\"count\"), lit(\" songs\"))\n",
    "    ).alias(\"result\")\n",
    ").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa949052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
